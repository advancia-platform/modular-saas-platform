# Logstash Configuration for AI DevOps Agent
# Processes logs from all AI DevOps Agent services

input {\n  # Docker container logs\n  beats {\n    port => 5044\n  }\n\n  # Direct log inputs from services\n  tcp {\n    port => 5000\n    codec => json_lines\n    tags => [\"tcp_input\"]\n  }\n\n  # Syslog for system logs\n  syslog {\n    port => 514\n    tags => [\"syslog\"]\n  }\n\n  # HTTP endpoint for webhook logs\n  http {\n    port => 8080\n    tags => [\"webhook\"]\n  }\n}\n\nfilter {\n  # Parse AI DevOps Agent specific logs\n  if [container_name] =~ /reasoning-engine/ {\n    grok {\n      match => { \"message\" => \"%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:level} %{DATA:logger} - %{GREEDYDATA:log_message}\" }\n    }\n    \n    mutate {\n      add_field => { \"service_type\" => \"reasoning-engine\" }\n      add_field => { \"ai_component\" => \"fintech-mappers\" }\n    }\n\n    # Extract AI analysis metrics\n    if [log_message] =~ /AI_ANALYSIS/ {\n      grok {\n        match => { \"log_message\" => \"AI_ANALYSIS: error_id=%{DATA:error_id} mapper=%{DATA:mapper} duration=%{NUMBER:analysis_duration:float} accuracy=%{NUMBER:accuracy:float}\" }\n      }\n      \n      mutate {\n        add_tag => [\"ai_analysis_metric\"]\n      }\n    }\n\n    # Extract fix generation metrics\n    if [log_message] =~ /FIX_GENERATED/ {\n      grok {\n        match => { \"log_message\" => \"FIX_GENERATED: error_id=%{DATA:error_id} strategy=%{DATA:deployment_strategy} risk_score=%{NUMBER:risk_score:float}\" }\n      }\n      \n      mutate {\n        add_tag => [\"fix_generation_metric\"]\n      }\n    }\n  }\n\n  if [container_name] =~ /execution-engine/ {\n    grok {\n      match => { \"message\" => \"%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:level} %{DATA:logger} - %{GREEDYDATA:log_message}\" }\n    }\n    \n    mutate {\n      add_field => { \"service_type\" => \"execution-engine\" }\n      add_field => { \"ai_component\" => \"deployment-automation\" }\n    }\n\n    # Extract deployment metrics\n    if [log_message] =~ /DEPLOYMENT/ {\n      grok {\n        match => { \"log_message\" => \"DEPLOYMENT: error_id=%{DATA:error_id} strategy=%{DATA:strategy} status=%{DATA:status} duration=%{NUMBER:deployment_duration:float}\" }\n      }\n      \n      mutate {\n        add_tag => [\"deployment_metric\"]\n      }\n    }\n\n    # Extract rollback metrics\n    if [log_message] =~ /ROLLBACK/ {\n      grok {\n        match => { \"log_message\" => \"ROLLBACK: error_id=%{DATA:error_id} reason=%{DATA:rollback_reason}\" }\n      }\n      \n      mutate {\n        add_tag => [\"rollback_metric\"]\n      }\n    }\n  }\n\n  # Parse PostgreSQL logs\n  if [container_name] =~ /postgres/ {\n    mutate {\n      add_field => { \"service_type\" => \"database\" }\n      add_field => { \"db_type\" => \"postgresql\" }\n    }\n\n    # Extract SQL query performance\n    if [message] =~ /duration:/ {\n      grok {\n        match => { \"message\" => \"duration: %{NUMBER:query_duration:float} ms\" }\n      }\n      \n      mutate {\n        add_tag => [\"db_performance_metric\"]\n      }\n    }\n  }\n\n  # Parse Redis logs\n  if [container_name] =~ /redis/ {\n    mutate {\n      add_field => { \"service_type\" => \"cache\" }\n      add_field => { \"cache_type\" => \"redis\" }\n    }\n  }\n\n  # Parse Prometheus logs\n  if [container_name] =~ /prometheus/ {\n    mutate {\n      add_field => { \"service_type\" => \"monitoring\" }\n      add_field => { \"monitoring_component\" => \"metrics\" }\n    }\n  }\n\n  # Parse Grafana logs\n  if [container_name] =~ /grafana/ {\n    mutate {\n      add_field => { \"service_type\" => \"monitoring\" }\n      add_field => { \"monitoring_component\" => \"visualization\" }\n    }\n  }\n\n  # Security log parsing\n  if \"security\" in [tags] or [log_message] =~ /SECURITY/ {\n    grok {\n      match => { \"log_message\" => \"SECURITY: event=%{DATA:security_event} severity=%{DATA:security_severity} user=%{DATA:user} ip=%{IP:client_ip}\" }\n    }\n    \n    mutate {\n      add_tag => [\"security_event\"]\n    }\n  }\n\n  # Error parsing\n  if [level] == \"ERROR\" or [level] == \"CRITICAL\" {\n    mutate {\n      add_tag => [\"error_log\"]\n    }\n\n    # Extract error details\n    grok {\n      match => { \"log_message\" => \"ERROR_DETECTED: type=%{DATA:error_type} severity=%{DATA:error_severity} component=%{DATA:component}\" }\n      tag_on_failure => [\"_grokparsefailure_error\"]\n    }\n  }\n\n  # Add common fields\n  mutate {\n    add_field => { \"[@metadata][index]\" => \"ai-devops-agent-%{+YYYY.MM.dd}\" }\n  }\n\n  # Convert timestamp\n  date {\n    match => [ \"timestamp\", \"ISO8601\" ]\n  }\n\n  # Remove temporary fields\n  mutate {\n    remove_field => [ \"host\", \"agent\", \"input\", \"ecs\" ]\n  }\n}\n\noutput {\n  # Send to Elasticsearch\n  elasticsearch {\n    hosts => [\"elasticsearch:9200\"]\n    index => \"%{[@metadata][index]}\"\n    template_name => \"ai-devops-agent\"\n    template => \"/usr/share/logstash/templates/ai-devops-agent.json\"\n    template_overwrite => true\n  }\n\n  # Debug output for development\n  if [@metadata][debug] {\n    stdout {\n      codec => rubydebug\n    }\n  }\n\n  # Send security events to separate index\n  if \"security_event\" in [tags] {\n    elasticsearch {\n      hosts => [\"elasticsearch:9200\"]\n      index => \"security-events-%{+YYYY.MM.dd}\"\n    }\n  }\n\n  # Send performance metrics to monitoring\n  if \"ai_analysis_metric\" in [tags] or \"deployment_metric\" in [tags] {\n    elasticsearch {\n      hosts => [\"elasticsearch:9200\"]\n      index => \"performance-metrics-%{+YYYY.MM.dd}\"\n    }\n  }\n}
